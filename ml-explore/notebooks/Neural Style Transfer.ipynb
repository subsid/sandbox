{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "directed-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by https://www.coursera.org/projects/neural-style-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prescribed-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import EagerTensor\n",
    "import pprint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(272) # DO NOT CHANGE THIS VALUE\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "img_size = 400\n",
    "vgg = tf.keras.applications.VGG19(include_top=False,\n",
    "                                  input_shape=(img_size, img_size, 3),\n",
    "                                  weights='pretrained-model/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "\n",
    "vgg.trainable = False\n",
    "pp.pprint(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = Image.open(\"~/Downloads/etsy_logo.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_content_cost(content_output, generated_output):\n",
    "    a_C = content_output[-1]\n",
    "    a_G = generated_output[-1]\n",
    "    \n",
    "    _, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
    "    \n",
    "    a_C_unrolled = tf.reshape(a_C, shape=[-1, n_H * n_W, n_C])\n",
    "    a_G_unrolled = tf.reshape(a_G, shape=[-1, n_H * n_W, n_C])\n",
    "    \n",
    "    J_content = (1 / (4 * n_H * n_W * n_C)) * tf.reduce_sum(\n",
    "        tf.square(\n",
    "            tf.subtract(a_C_unrolled, a_G_unrolled)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return J_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "a_C = tf.random.normal([1, 1, 4, 4, 3], mean=1, stddev=4)\n",
    "a_G = tf.random.normal([1, 1, 4, 4, 3], mean=1, stddev=4)\n",
    "J_content = compute_content_cost(a_C, a_G)\n",
    "J_content_0 = compute_content_cost(a_C, a_C)\n",
    "assert type(J_content) == EagerTensor, \"Use the tensorflow function\"\n",
    "assert np.isclose(J_content_0, 0.0), \"Wrong value. compute_content_cost(A, A) must be 0\"\n",
    "assert np.isclose(J_content, 7.0568767), f\"Wrong value. Expected {7.0568767},  current{J_content}\"\n",
    "\n",
    "print(\"J_content = \" + str(J_content))\n",
    "\n",
    "ll = tf.keras.layers.Dense(8, activation='relu', input_shape=(1, 4, 4, 3))\n",
    "model_tmp = tf.keras.models.Sequential()\n",
    "model_tmp.add(ll)\n",
    "try:\n",
    "    compute_content_cost(ll.output, ll.output)\n",
    "    print(\"\\033[92mAll tests passed\")\n",
    "except Exception as inst:\n",
    "    print(\"\\n\\033[91mDon't use the numpy API inside compute_content_cost\\n\")\n",
    "    print(inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(A):\n",
    "    GA = tf.matmul(A, A, transpose_b = True)\n",
    "\n",
    "    return GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer_style_cost(a_S, a_G):\n",
    "    # Retrieve dimensions from a_G (≈1 line)\n",
    "    _, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
    "    \n",
    "    # Reshape the images from (n_H * n_W, n_C) to have them of shape (n_C, n_H * n_W) (≈2 lines)\n",
    "    a_S = tf.reshape(tf.transpose(a_S, perm=[0,3,1,2]), (-1, n_C, n_H * n_W))\n",
    "    a_G = tf.reshape(tf.transpose(a_G, perm=[0,3,1,2]), (-1, n_C, n_H * n_W))\n",
    "\n",
    "\n",
    "    # Computing gram_matrices for both images S and G (≈2 lines)\n",
    "    GS = tf.matmul(a_S, a_S, transpose_b = True)\n",
    "    GG = tf.matmul(a_G, a_G, transpose_b = True)\n",
    "\n",
    "    # Computing the loss (≈1 line)\n",
    "    J_style_layer = ((1 / (4 * n_C**2 * (n_H * n_W)**2)) *\n",
    "        (tf.reduce_sum(\n",
    "            tf.square(\n",
    "                tf.subtract(GS, GG)\n",
    "            )\n",
    "        )))\n",
    "    #J_style_layer = None\n",
    "    \n",
    "    return J_style_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_style_cost(style_image_output, generated_image_output, STYLE_LAYERS=STYLE_LAYERS):    \n",
    "    # initialize the overall style cost\n",
    "    J_style = 0\n",
    "\n",
    "    # Set a_S to be the hidden layer activation from the layer we have selected.\n",
    "    # The last element of the array contains the content layer image, which must not be used.\n",
    "    a_S = style_image_output[:-1]\n",
    "\n",
    "    # Set a_G to be the output of the choosen hidden layers.\n",
    "    # The last element of the list contains the content layer image which must not be used.\n",
    "    a_G = generated_image_output[:-1]\n",
    "    for i, weight in zip(range(len(a_S)), STYLE_LAYERS):  \n",
    "        # Compute style_cost for the current layer\n",
    "        J_style_layer = compute_layer_style_cost(a_S[i], a_G[i])\n",
    "\n",
    "        # Add weight * J_style_layer of this layer to overall style cost\n",
    "        J_style += weight[1] * J_style_layer\n",
    "\n",
    "    return J_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def total_cost(J_content, J_style, alpha = 10, beta = 40):\n",
    "    \"\"\"\n",
    "    Computes the total cost function\n",
    "    \"\"\"\n",
    "    J = alpha * J_content + beta * J_style\n",
    "\n",
    "\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(generated_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # In this function you must use the precomputed encoded images a_S and a_C\n",
    "        # Compute a_G as the vgg_model_outputs for the current generated image\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        \n",
    "        #(1 line)\n",
    "        a_G = vgg_model_outputs(generated_image)\n",
    "        \n",
    "        # Compute the style cost\n",
    "        #(1 line)\n",
    "        J_style = compute_style_cost(a_S, a_G)\n",
    "\n",
    "        #(2 lines)\n",
    "        # Compute the content cost\n",
    "        J_content = compute_content_cost(a_C, a_G)\n",
    "        # Compute the total cost\n",
    "        J = total_cost(J_content, J_style, alpha = 10, beta = 40)\n",
    "        \n",
    "        ### END CODE HERE\n",
    "        \n",
    "    grad = tape.gradient(J, generated_image)\n",
    "\n",
    "    optimizer.apply_gradients([(grad, generated_image)])\n",
    "    generated_image.assign(clip_0_1(generated_image))\n",
    "    # For grading purposes\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "for i in range(epochs):\n",
    "    train_step(generated_image)\n",
    "    if i % 250 == 0:\n",
    "        print(f\"Epoch {i} \")\n",
    "    if i % 250 == 0:\n",
    "        image = tensor_to_image(generated_image)\n",
    "        imshow(image)\n",
    "        image.save(f\"output/image_{i}.jpg\")\n",
    "        plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
